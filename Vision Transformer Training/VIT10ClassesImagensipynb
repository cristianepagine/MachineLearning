{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNESCdaAky88Dd1bPOAEyfi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import os\n","import random\n","import shutil\n","\n","# Diretórios\n","original_dir = \"/content/drive/MyDrive/Colab Notebooks/Base\"\n","train_dir = os.path.join(original_dir, \"train\")\n","val_dir = os.path.join(original_dir, \"val\")\n","\n","os.makedirs(train_dir, exist_ok=True)\n","os.makedirs(val_dir, exist_ok=True)\n","\n","random.seed(42)  # Reprodutibilidade\n","\n","# Mover 20% de cada classe para val/\n","for class_name in os.listdir(original_dir):\n","    class_path = os.path.join(original_dir, class_name)\n","    if not os.path.isdir(class_path) or class_name in [\"train\", \"val\"]:\n","        continue\n","\n","    images = os.listdir(class_path)\n","    random.shuffle(images)\n","    split_idx = int(0.8 * len(images))\n","    train_imgs = images[:split_idx]\n","    val_imgs = images[split_idx:]\n","\n","    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n","    os.makedirs(os.path.join(val_dir, class_name), exist_ok=True)\n","\n","    for img in train_imgs:\n","        shutil.copy(os.path.join(class_path, img), os.path.join(train_dir, class_name, img))\n","    for img in val_imgs:\n","        shutil.copy(os.path.join(class_path, img), os.path.join(val_dir, class_name, img))\n","\n","print(\"Divisão concluída.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f46k_qwIP9HO","executionInfo":{"status":"ok","timestamp":1750253701208,"user_tz":180,"elapsed":11348,"user":{"displayName":"Cristiane Pagine","userId":"14007945378641161929"}},"outputId":"341854c3-9b43-4a35-c986-9c3d97728c6f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Divisão concluída.\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import torch\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","from torchvision.transforms import Compose, Resize, ToTensor, Normalize, RandomHorizontalFlip\n","from transformers import ViTForImageClassification, ViTImageProcessor\n","from torch import nn, optim\n","from sklearn.metrics import classification_report\n","\n","# Configurações\n","train_dir = \"/content/drive/MyDrive/Colab Notebooks/Base/train\"\n","val_dir = \"/content/drive/MyDrive/Colab Notebooks/Base/val\"\n","batch_size = 16\n","num_epochs = 10\n","learning_rate = 1e-4\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Usando dispositivo: {device}\")\n","\n","# Normalização do ViT\n","feature_extractor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n","normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n","\n","transform = Compose([\n","    Resize((224, 224)),\n","    RandomHorizontalFlip(),\n","    ToTensor(),\n","    normalize\n","])\n","\n","# Datasets e dataloaders\n","train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n","val_dataset   = datasets.ImageFolder(root=val_dir, transform=transform)\n","num_classes = len(train_dataset.classes)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader   = DataLoader(val_dataset, batch_size=batch_size)\n","\n","# Modelo ViT\n","model = ViTForImageClassification.from_pretrained(\n","    'google/vit-base-patch16-224',\n","    num_labels=num_classes,\n","    ignore_mismatched_sizes=True\n",")\n","model.classifier = nn.Linear(model.config.hidden_size, num_classes)\n","model.to(device)\n","\n","# Otimizador e função perda\n","optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Treinamento\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0\n","    for imgs, labels in train_loader:\n","        imgs, labels = imgs.to(device), labels.to(device)\n","\n","        outputs = model(imgs)\n","        loss = criterion(outputs.logits, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    avg_loss = running_loss / len(train_loader)\n","    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n","\n","    # Validação\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for imgs, labels in val_loader:\n","            imgs, labels = imgs.to(device), labels.to(device)\n","            outputs = model(imgs)\n","            _, preds = torch.max(outputs.logits, dim=1)\n","            correct += (preds == labels).sum().item()\n","            total += labels.size(0)\n","    val_acc = correct / total\n","    print(f\"Val Accuracy: {val_acc:.4f}\")\n","\n","# Avaliação final com relatório\n","model.eval()\n","all_preds = []\n","all_labels = []\n","\n","with torch.no_grad():\n","    for imgs, labels in val_loader:\n","        imgs = imgs.to(device)\n","        outputs = model(imgs)\n","        _, preds = torch.max(outputs.logits, dim=1)\n","        all_preds.extend(preds.cpu().numpy())\n","        all_labels.extend(labels.numpy())\n","\n","print(\"\\nRelatório de Classificação:\\n\")\n","print(classification_report(all_labels, all_preds, target_names=val_dataset.classes))\n","\n","# Salvar modelo\n","torch.save(model.state_dict(), f\"vit_finetuned_acc{val_acc:.2f}.pth\")\n","print(f\"Modelo salvo: vit_finetuned_acc{val_acc:.2f}.pth\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4CISV_bzP-5o","executionInfo":{"status":"ok","timestamp":1750254076224,"user_tz":180,"elapsed":346715,"user":{"displayName":"Cristiane Pagine","userId":"14007945378641161929"}},"outputId":"3b501555-984f-4c1b-f48e-fc405c94214d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Usando dispositivo: cuda\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n","- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n","- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10 - Loss: 0.3672\n","Val Accuracy: 1.0000\n","Epoch 2/10 - Loss: 0.0259\n","Val Accuracy: 0.9950\n","Epoch 3/10 - Loss: 0.0035\n","Val Accuracy: 1.0000\n","Epoch 4/10 - Loss: 0.0020\n","Val Accuracy: 1.0000\n","Epoch 5/10 - Loss: 0.0014\n","Val Accuracy: 1.0000\n","Epoch 6/10 - Loss: 0.0011\n","Val Accuracy: 1.0000\n","Epoch 7/10 - Loss: 0.0009\n","Val Accuracy: 0.9950\n","Epoch 8/10 - Loss: 0.0007\n","Val Accuracy: 0.9950\n","Epoch 9/10 - Loss: 0.0006\n","Val Accuracy: 0.9950\n","Epoch 10/10 - Loss: 0.0005\n","Val Accuracy: 0.9950\n","\n","Relatório de Classificação:\n","\n","              precision    recall  f1-score   support\n","\n","     cavalos       1.00      1.00      1.00        20\n","      comida       1.00      1.00      1.00        20\n","        dino       1.00      1.00      1.00        20\n","    elefante       1.00      1.00      1.00        20\n","      flores       1.00      1.00      1.00        20\n","     humanos       0.95      1.00      0.98        20\n","   montanhas       1.00      1.00      1.00        20\n","       obras       1.00      1.00      1.00        20\n","      onibus       1.00      1.00      1.00        20\n","       praia       1.00      0.95      0.97        20\n","\n","    accuracy                           0.99       200\n","   macro avg       1.00      0.99      0.99       200\n","weighted avg       1.00      0.99      0.99       200\n","\n","Modelo salvo: vit_finetuned_acc0.99.pth\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","import pandas as pd\n","\n","# Gera a matriz de confusão\n","cm = confusion_matrix(all_labels, all_preds)\n","\n","# Transforma em DataFrame para facilitar leitura\n","cm_df = pd.DataFrame(cm, index=val_dataset.classes, columns=val_dataset.classes)\n","\n","# Exibe no notebook\n","print(\"Matriz de Confusão:\\n\")\n","print(cm_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a7ARrl2AShir","executionInfo":{"status":"ok","timestamp":1750254387388,"user_tz":180,"elapsed":30,"user":{"displayName":"Cristiane Pagine","userId":"14007945378641161929"}},"outputId":"8245edd4-9345-4abf-947d-5b73bfad27f8"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Matriz de Confusão:\n","\n","           cavalos  comida  dino  elefante  flores  humanos  montanhas  obras  \\\n","cavalos         20       0     0         0       0        0          0      0   \n","comida           0      20     0         0       0        0          0      0   \n","dino             0       0    20         0       0        0          0      0   \n","elefante         0       0     0        20       0        0          0      0   \n","flores           0       0     0         0      20        0          0      0   \n","humanos          0       0     0         0       0       20          0      0   \n","montanhas        0       0     0         0       0        0         20      0   \n","obras            0       0     0         0       0        0          0     20   \n","onibus           0       0     0         0       0        0          0      0   \n","praia            0       0     0         0       0        1          0      0   \n","\n","           onibus  praia  \n","cavalos         0      0  \n","comida          0      0  \n","dino            0      0  \n","elefante        0      0  \n","flores          0      0  \n","humanos         0      0  \n","montanhas       0      0  \n","obras           0      0  \n","onibus         20      0  \n","praia           0     19  \n"]}]},{"cell_type":"code","source":["# 1. Montar o Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 2. Imports\n","import torch\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","from torchvision.transforms import Compose, Resize, ToTensor, Normalize, RandomHorizontalFlip\n","from transformers import ViTForImageClassification, ViTImageProcessor\n","from torch import nn, optim\n","from sklearn.metrics import classification_report, confusion_matrix\n","import pandas as pd\n","\n","# 3. Configurações\n","train_dir = \"/content/drive/MyDrive/Colab Notebooks/Base/train\"\n","val_dir   = \"/content/drive/MyDrive/Colab Notebooks/Base/val\"\n","batch_size = 16\n","num_epochs = 10\n","learning_rate = 1e-4\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Usando dispositivo: {device}\")\n","\n","# 4. Transformações\n","feature_extractor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n","normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n","\n","transform = Compose([\n","    Resize((224, 224)),\n","    RandomHorizontalFlip(),\n","    ToTensor(),\n","    normalize\n","])\n","\n","# 5. Datasets e Loaders\n","train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n","val_dataset   = datasets.ImageFolder(root=val_dir, transform=transform)\n","num_classes = len(train_dataset.classes)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader   = DataLoader(val_dataset, batch_size=batch_size)\n","\n","# 6. Modelo ViT\n","model = ViTForImageClassification.from_pretrained(\n","    'google/vit-base-patch16-224',\n","    num_labels=num_classes,\n","    ignore_mismatched_sizes=True\n",")\n","model.classifier = nn.Linear(model.config.hidden_size, num_classes)\n","model.to(device)\n","\n","# 7. Otimizador e Perda\n","optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n","criterion = nn.CrossEntropyLoss()\n","\n","# 8. Treinamento\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0\n","    for imgs, labels in train_loader:\n","        imgs, labels = imgs.to(device), labels.to(device)\n","\n","        outputs = model(imgs)\n","        loss = criterion(outputs.logits, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","    avg_loss = running_loss / len(train_loader)\n","    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n","\n","    # Validação\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    all_preds = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for imgs, labels in val_loader:\n","            imgs, labels = imgs.to(device), labels.to(device)\n","            outputs = model(imgs)\n","            _, preds = torch.max(outputs.logits, dim=1)\n","            correct += (preds == labels).sum().item()\n","            total += labels.size(0)\n","\n","            all_preds.extend(preds.cpu().tolist())\n","            all_labels.extend(labels.cpu().tolist())\n","\n","    val_acc = correct / total\n","    print(f\"Val Accuracy: {val_acc:.4f}\")\n","\n","# 9. Relatório de Classificação\n","print(\"\\nRelatório de Classificação:\")\n","print(classification_report(all_labels, all_preds, target_names=val_dataset.classes))\n","\n","# 10. Matriz de confusão (sem plotar)\n","cm = confusion_matrix(all_labels, all_preds)\n","cm_df = pd.DataFrame(cm, index=val_dataset.classes, columns=val_dataset.classes)\n","print(\"\\nMatriz de Confusão:\")\n","print(cm_df)\n","\n","# 11. Salvar modelo\n","torch.save(model.state_dict(), \"vit_finetuned.pth\")\n","print(\"Modelo salvo como vit_finetuned.pth\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wCoryRG7Y4LQ","executionInfo":{"status":"ok","timestamp":1750257163817,"user_tz":180,"elapsed":345945,"user":{"displayName":"Cristiane Pagine","userId":"14007945378641161929"}},"outputId":"15c3f1f1-7bb9-4258-c0aa-7b38c907f964"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Usando dispositivo: cuda\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n","- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n","- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10 - Loss: 0.3668\n","Val Accuracy: 1.0000\n","Epoch 2/10 - Loss: 0.0093\n","Val Accuracy: 0.9950\n","Epoch 3/10 - Loss: 0.0033\n","Val Accuracy: 0.9950\n","Epoch 4/10 - Loss: 0.0019\n","Val Accuracy: 0.9950\n","Epoch 5/10 - Loss: 0.0014\n","Val Accuracy: 0.9950\n","Epoch 6/10 - Loss: 0.0011\n","Val Accuracy: 0.9950\n","Epoch 7/10 - Loss: 0.0009\n","Val Accuracy: 0.9950\n","Epoch 8/10 - Loss: 0.0007\n","Val Accuracy: 0.9950\n","Epoch 9/10 - Loss: 0.0006\n","Val Accuracy: 0.9950\n","Epoch 10/10 - Loss: 0.0005\n","Val Accuracy: 0.9950\n","\n","Relatório de Classificação:\n","              precision    recall  f1-score   support\n","\n","     cavalos       1.00      1.00      1.00        20\n","      comida       1.00      1.00      1.00        20\n","        dino       1.00      1.00      1.00        20\n","    elefante       1.00      1.00      1.00        20\n","      flores       1.00      1.00      1.00        20\n","     humanos       0.95      1.00      0.98        20\n","   montanhas       1.00      1.00      1.00        20\n","       obras       1.00      1.00      1.00        20\n","      onibus       1.00      1.00      1.00        20\n","       praia       1.00      0.95      0.97        20\n","\n","    accuracy                           0.99       200\n","   macro avg       1.00      0.99      0.99       200\n","weighted avg       1.00      0.99      0.99       200\n","\n","\n","Matriz de Confusão:\n","           cavalos  comida  dino  elefante  flores  humanos  montanhas  obras  \\\n","cavalos         20       0     0         0       0        0          0      0   \n","comida           0      20     0         0       0        0          0      0   \n","dino             0       0    20         0       0        0          0      0   \n","elefante         0       0     0        20       0        0          0      0   \n","flores           0       0     0         0      20        0          0      0   \n","humanos          0       0     0         0       0       20          0      0   \n","montanhas        0       0     0         0       0        0         20      0   \n","obras            0       0     0         0       0        0          0     20   \n","onibus           0       0     0         0       0        0          0      0   \n","praia            0       0     0         0       0        1          0      0   \n","\n","           onibus  praia  \n","cavalos         0      0  \n","comida          0      0  \n","dino            0      0  \n","elefante        0      0  \n","flores          0      0  \n","humanos         0      0  \n","montanhas       0      0  \n","obras           0      0  \n","onibus         20      0  \n","praia           0     19  \n","Modelo salvo como vit_finetuned.pth\n"]}]}]}
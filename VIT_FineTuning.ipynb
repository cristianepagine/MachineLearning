{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9zF84UVVvj0v","executionInfo":{"status":"aborted","timestamp":1750161020104,"user_tz":180,"elapsed":77787,"user":{"displayName":"Cristiane Pagine","userId":"14007945378641161929"}}},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import layers, models, datasets\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Carregar e pré-processar os dados CIFAR-10\n","(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n","\n","# Normalizar os valores de pixel para o intervalo [0, 1]\n","train_images, test_images = train_images / 255.0, test_images / 255.0\n","\n","# Converter rótulos para one-hot encoding (necessário para categorical_crossentropy)\n","# Se você usar sparse_categorical_crossentropy, não precisa desta linha\n","# train_labels_one_hot = to_categorical(train_labels, num_classes=10)\n","# test_labels_one_hot = to_categorical(test_labels, num_classes=10)\n","\n","print(f\"Shape das imagens de treino: {train_images.shape}\")\n","print(f\"Shape dos rótulos de treino: {train_labels.shape}\") # ou train_labels_one_hot.shape se one-hot\n","\n","# Construir a CNN do zero\n","model_cnn_scratch = models.Sequential([\n","    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n","    layers.MaxPooling2D((2, 2)),\n","    layers.Conv2D(64, (3, 3), activation='relu'),\n","    layers.MaxPooling2D((2, 2)),\n","    layers.Conv2D(64, (3, 3), activation='relu'),\n","    layers.Flatten(),\n","    layers.Dense(64, activation='relu'),\n","    layers.Dense(10, activation='softmax') # 10 classes para CIFAR-10\n","])\n","\n","# Compilar o modelo\n","model_cnn_scratch.compile(optimizer='adam',\n","                          loss='sparse_categorical_crossentropy', # Use 'categorical_crossentropy' se usar one-hot encoding\n","                          metrics=['accuracy'])\n","\n","# Resumo do modelo\n","model_cnn_scratch.summary()\n","\n","# Treinar o modelo\n","history_cnn_scratch = model_cnn_scratch.fit(train_images, train_labels,\n","                                            epochs=10, # Você pode ajustar o número de épocas\n","                                            validation_data=(test_images, test_labels))\n","\n","# Avaliar o modelo\n","test_loss_cnn_scratch, test_acc_cnn_scratch = model_cnn_scratch.evaluate(test_images, test_labels, verbose=2)\n","print(f\"\\nAcurácia da CNN do zero no teste: {test_acc_cnn_scratch * 100:.2f}%\")\n","\n","# Previsões para a matriz de confusão\n","y_pred_cnn_scratch = np.argmax(model_cnn_scratch.predict(test_images), axis=1)\n","conf_matrix_cnn_scratch = confusion_matrix(test_labels, y_pred_cnn_scratch)\n","\n","print(\"\\nMatriz de Confusão (CNN do Zero):\")\n","print(conf_matrix_cnn_scratch)\n","\n","# Opcional: Visualizar matriz de confusão\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix_cnn_scratch, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=[f'Classe {i}' for i in range(10)],\n","            yticklabels=[f'Classe {i}' for i in range(10)])\n","plt.title('Matriz de Confusão - CNN do Zero')\n","plt.ylabel('Rótulo Verdadeiro')\n","plt.xlabel('Rótulo Previsto')\n","plt.show()\n","\n","# Guarde test_acc_cnn_scratch e conf_matrix_cnn_scratch para o relatório"]},{"cell_type":"markdown","metadata":{"id":"I2JqjIpv0ji9"},"source":["Estratégia 2: Extrator de Características com Rede Pré-Treinada\n","Aqui, usaremos uma rede pré-treinada (VGG16 é um bom ponto de partida) para extrair características e, em seguida, treinar um classificador simples (como uma Dense ou SVC) sobre essas características."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"yT8N7uc-0wV1","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1750161212470,"user_tz":180,"elapsed":104978,"user":{"displayName":"Cristiane Pagine","userId":"14007945378641161929"}},"outputId":"0c91cecb-f911-4b4d-d38e-d54d2e1b6085"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n","Collecting evaluate\n","  Using cached evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.2)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Using cached evaluate-0.4.3-py3-none-any.whl (84 kB)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, evaluate\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed evaluate-0.4.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["nvidia"]},"id":"a0ca94cff7fb48f6892e35407b179fc1"}},"metadata":{}},{"output_type":"error","ename":"AttributeError","evalue":"module 'torch.cuda' has no attribute 'enable_gpu'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-924879486>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Certifique-se de que o PyTorch está usando a GPU (se disponível)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Alterado de torch.cuda.is_available() para torch.cuda.enable_gpu()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Usando dispositivo: {device}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'torch.cuda' has no attribute 'enable_gpu'"]}],"source":["# --- 0. Instalação e Importações Necessárias ---\n","# Execute esta célula no seu Google Colab\n","!pip install transformers datasets accelerate evaluate scikit-learn matplotlib seaborn torch torchvision opencv-python Pillow\n","\n","import tensorflow as tf\n","from tensorflow.keras import datasets, layers, models\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from transformers import ViTFeatureExtractor, ViTForImageClassification, TrainingArguments, Trainer\n","from datasets import load_dataset, Dataset as HfDataset # Importar Dataset do Hugging Face\n","\n","# Certifique-se de que o PyTorch está usando a GPU (se disponível)\n","device = torch.device(\"cuda\" if torch.cuda.enable_gpu() else \"cpu\") # Alterado de torch.cuda.is_available() para torch.cuda.enable_gpu()\n","print(f\"Usando dispositivo: {device}\")\n","\n","# --- 1. Carregar e Preparar a Base de Dados (Simulação com CIFAR-10) ---\n","# Como você mencionou \"10 classes, 1000 imagens\",\n","# vamos SIMULAR essa situação pegando APENAS 100 imagens por classe do CIFAR-10.\n","# Em um cenário real, você carregaria suas próprias 1000 imagens.\n","\n","(train_images_cifar, train_labels_cifar), (test_images_cifar, test_labels_cifar) = datasets.cifar10.load_data()\n","\n","# Filtrar para ter 100 imagens por classe para simular sua base de 1000 imagens de treino\n","num_samples_per_class = 100\n","train_indices = []\n","for i in range(10): # Para cada uma das 10 classes\n","    class_indices = np.where(train_labels_cifar.flatten() == i)[0]\n","    train_indices.extend(np.random.choice(class_indices, num_samples_per_class, replace=False))\n","\n","train_images_simulated = train_images_cifar[train_indices]\n","train_labels_simulated = train_labels_cifar[train_indices]\n","\n","# Usaremos o conjunto de teste original para avaliação, pois ele simula dados \"não vistos\"\n","test_images_simulated = test_images_cifar\n","test_labels_simulated = test_labels_cifar\n","\n","print(f\"Total de imagens de treino simuladas: {len(train_images_simulated)}\")\n","print(f\"Total de imagens de teste simuladas: {len(test_images_simulated)}\")\n","\n","# Mapear IDs de classe para nomes de classe (para melhor visualização)\n","id2label = {i: str(i) for i in range(10)}\n","label2id = {str(i): i for i in range(10)}\n","\n","# --- 2. Pré-processamento e Aumento de Dados para ViT ---\n","# O ViT requer imagens de 224x224 (para o modelo vit-base-patch16-224-in21k).\n","# As transformações são CRUCIAIS para bases pequenas.\n","\n","# Carrega o feature extractor do ViT pré-treinado\n","# Ele cuidará da normalização específica (mean/std) e do redimensionamento para 224x224\n","feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n","\n","# Definir as transformações de Data Augmentation para o treino\n","# Essas são transformações AVANÇADAS e importantes para bases pequenas!\n","train_transforms = transforms.Compose([\n","    transforms.ToPILImage(), # Converte numpy array para PIL Image para as transforms\n","    transforms.RandomResizedCrop(feature_extractor.size[\"height\"]), # Alterado para acessar o valor do dicionário\n","    transforms.RandomHorizontalFlip(), # Espelhamento horizontal\n","    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1), # Variação de cor\n","    transforms.ToTensor(), # Converte para tensor PyTorch\n","    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std), # Normalização específica do ViT\n","])\n","\n","# Definir as transformações para o conjunto de validação/teste (apenas redimensionamento e normalização)\n","test_transforms = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize(feature_extractor.size[\"height\"]), # Alterado para acessar o valor do dicionário\n","    transforms.CenterCrop(feature_extractor.size[\"height\"]), # Alterado para acessar o valor do dicionário\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n","])\n","\n","# Criar uma classe Dataset customizada para PyTorch\n","class ImageClassificationDataset(Dataset):\n","    def __init__(self, images, labels, transform=None):\n","        self.images = images\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image = self.images[idx]\n","        label = self.labels[idx]\n","        if self.transform:\n","            image = self.transform(image)\n","        return {\"pixel_values\": image, \"labels\": torch.tensor(label, dtype=torch.long).flatten()}\n","\n","# Criar os datasets PyTorch\n","train_dataset = ImageClassificationDataset(train_images_simulated, train_labels_simulated, train_transforms)\n","test_dataset = ImageClassificationDataset(test_images_simulated, test_labels_simulated, test_transforms)\n","\n","# Converter para o formato Dataset do Hugging Face (opcional, mas facilita o Trainer)\n","train_dataset_hf = HfDataset.from_dict({\"pixel_values\": [np.array(img) for img in train_dataset.images], \"labels\": train_dataset.labels.flatten().tolist()})\n","test_dataset_hf = HfDataset.from_dict({\"pixel_values\": [np.array(img) for img in test_dataset.images], \"labels\": test_dataset.labels.flatten().tolist()})\n","\n","# Aplicar o pré-processamento do feature_extractor\n","def preprocess_dataset(examples):\n","    # O feature_extractor aceita imagens no formato PIL Image ou numpy array\n","    images = [tf.keras.preprocessing.image.array_to_img(img) for img in examples[\"pixel_values\"]]\n","    inputs = feature_extractor(images, return_tensors=\"pt\")\n","    inputs[\"labels\"] = examples[\"labels\"]\n","    return inputs\n","\n","# Não, é melhor usar o Dataset customizado com transforms do torchvision para ter mais controle sobre augmentation\n","# train_dataset_processed = train_dataset_hf.map(preprocess_dataset, batched=True)\n","# test_dataset_processed = test_dataset_hf.map(preprocess_dataset, batched=True)\n","# train_dataset_processed.set_format(type=\"torch\", columns=[\"pixel_values\", \"labels\"])\n","# test_dataset_processed.set_format(type=\"torch\", columns=[\"pixel_values\", \"labels\"])\n","\n","\n","# --- 3. Carregar o Modelo ViT para Fine-tuning ---\n","# Carregar o modelo ViT pré-treinado (com cabeça de classificação para 10 classes)\n","# O `num_labels` ajusta automaticamente a última camada de classificação.\n","model = ViTForImageClassification.from_pretrained(\n","    'google/vit-base-patch16-224-in21k',\n","    num_labels=10,\n","    id2label=id2label,\n","    label2id=label2id,\n","    ignore_mismatched_sizes=True # Permite ajustar a camada de saída\n",")\n","\n","# Enviar o modelo para a GPU\n","model.to(device)\n","\n","# --- 4. Configurar o Treinador (Trainer do Hugging Face) ---\n","# Métricas de avaliação\n","import evaluate\n","metric = evaluate.load(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","# Argumentos de treinamento\n","training_args = TrainingArguments(\n","    output_dir=\"./vit-fine-tuned-cifar10\", # Diretório para salvar checkpoints\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=10, # Aumente se necessário, mas monitore overfitting\n","    learning_rate=2e-5, # Taxa de aprendizado pequena para fine-tuning\n","    evaluation_strategy=\"epoch\", # Avaliar a cada época\n","    logging_dir=\"./logs\",\n","    logging_steps=100,\n","    save_strategy=\"epoch\", # Salvar checkpoint a cada época\n","    load_best_model_at_end=True, # Carregar o melhor modelo no final\n","    metric_for_best_model=\"accuracy\",\n","    report_to=\"none\", # Não reportar para ferramentas externas (wandb, etc.)\n",")\n","\n","# Criar o Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset, # Usar o dataset PyTorch customizado\n","    eval_dataset=test_dataset,   # Usar o dataset PyTorch customizado\n","    compute_metrics=compute_metrics,\n","    data_collator=lambda data: {\n","        'pixel_values': torch.stack([d['pixel_values'] for d in data]),\n","        'labels': torch.stack([d['labels'] for d in data])\n","    }\n",")\n","\n","# --- 5. Realizar o Fine-tuning ---\n","print(\"\\n--- Iniciando o Fine-tuning do ViT ---\")\n","trainer.train()\n","\n","# --- 6. Avaliar o Modelo Final ---\n","print(\"\\n--- Avaliando o Modelo Final ---\")\n","results = trainer.evaluate(test_dataset)\n","print(f\"Resultados da Avaliação: {results}\")\n","\n","# Fazer previsões no conjunto de teste para a matriz de confusão\n","predictions_raw = trainer.predict(test_dataset)\n","y_pred = np.argmax(predictions_raw.predictions, axis=1)\n","y_true = np.array([example[\"labels\"].item() for example in test_dataset]) # Extrair rótulos verdadeiros\n","\n","conf_matrix = confusion_matrix(y_true, y_pred)\n","accuracy = accuracy_score(y_true, y_pred)\n","\n","print(f\"\\nAcurácia do ViT Fine-Tuning no teste: {accuracy * 100:.2f}%\")\n","print(\"\\nMatriz de Confusão (ViT Fine-Tuning):\")\n","print(conf_matrix)\n","\n","# Visualização da Matriz de Confusão\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=list(id2label.values()), yticklabels=list(id2label.values()))\n","plt.xlabel('Predito')\n","plt.ylabel('Verdadeiro')\n","plt.title('Matriz de Confusão (ViT Fine-Tuning)')\n","plt.show()\n","\n","# --- Dicas Adicionais para Melhorar (e Lidar com Poucos Dados) ---\n","print(\"\\n--- Dicas Adicionais para Melhorar (e Lidar com Poucos Dados) ---\")\n","print(\"1. **Aumentar o número de épocas:** Com um dataset pequeno, pode ser que o modelo precise de mais épocas para convergir, mas cuidado com overfitting.\")\n","print(\"2. **Ajustar a Learning Rate:** Experimente valores um pouco menores (e.g., 1e-6) ou use um scheduler de learning rate mais sofisticado.\")\n","print(\"3. **Explorar mais Data Augmentation:** Se a acurácia não for satisfatória, adicione mais transformações complexas (ex: AutoAugment/RandAugment).\")\n","print(\"4. **Tentar descongelar algumas camadas:** Se a acurácia estiver baixa, você pode tentar descongelar as últimas camadas do ViT (deixando `model.vit.encoder.layer[-N:].trainable = True`) e continuar o fine-tuning com uma learning rate ainda menor.\")\n","print(\"5. **Usar um ViT menor:** Para bases muito pequenas, um ViT menor (ex: `google/vit-tiny-patch16-224-in21k`) pode ser menos propenso a overfitting, embora possa ter menor capacidade.\")"]}],"metadata":{"colab":{"provenance":[{"file_id":"1AX_lgQWYD3nXkmlGfYkf--7Yn-X7v4L1","timestamp":1750160874146},{"file_id":"1d9Rly9i8YuPF3pju0Pt0pvQLGMkDNvQs","timestamp":1750078677043},{"file_id":"1x7Xs8RbYZYEDILO1l9-BLOv35-pMeh7q","timestamp":1748008166142},{"file_id":"1yd560qVdaRPkjA7ytmvy0n4yeH7Q_iyf","timestamp":1747833861057}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}
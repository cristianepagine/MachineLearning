{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1d9Rly9i8YuPF3pju0Pt0pvQLGMkDNvQs","timestamp":1750076330474},{"file_id":"1x7Xs8RbYZYEDILO1l9-BLOv35-pMeh7q","timestamp":1748008166142},{"file_id":"1yd560qVdaRPkjA7ytmvy0n4yeH7Q_iyf","timestamp":1747833861057}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Treinamento e avaliação de modelos para problema de classificação de câncer de mama a partir de imagem digital, em maligno ou benigno. A base foi criada por pesquisadores da Universidade de Wisconsin nos Estados Unidos.\n","\n","Link para a base no Scikit Learn:\n","https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n","\n","Qtde de atributos igual a 31, sendo:\n","classe (0=maligno ou 1=benigno)\n","30 atributos contínuos extraídos do núcleo das células em imagem de câncer de mama.\n","\n","Tipo dos atributos: numéricos\n","Qtde de instâncias: 569 (212 malignos e 357 benignos)\n","Protocolo experimental a ser utilizado: validação cruzada com 5 folds.\n","Indutores a serem avaliados: Árvores de decisão, KNN, Naive Bayes, SVM, MLP e ensembles (Random Forest, Bagging, AdaBoosting, XGBoosting).\n"],"metadata":{"id":"NC-D5LX2ccnm"}},{"cell_type":"markdown","source":[],"metadata":{"id":"ck8wCPEJcg_s"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"9zF84UVVvj0v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750077238925,"user_tz":180,"elapsed":15669,"user":{"displayName":"Cristiane Pagine","userId":"14007945378641161929"}},"outputId":"bb78d636-36fa-4da2-d11e-b9fa89b098cd"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["---\n","## Tabela de Resultados (Todos os Regressores)\n","\n","| Indutor | Coeficiente de Determinação (R2) | Erro Médio Absoluto (MAE) |\n","|:--------------------|:------------------------------------|:--------------------------|\n","| Árvore de Regressão | -0.1343 | 63.5188 |\n","| KNN | 0.3903 | 46.7387 |\n","| SVR | 0.1495 | 58.8854 |\n","| **MLP** | **0.4497** | **45.1405** |\n","| Random Forest | 0.4299 | 46.6673 |\n","| Bagging | 0.3746 | 48.7749 |\n","| XGBoosting | 0.4164 | 47.3211 |\n","\n","---\n","## B.1) Parâmetros do Melhor Modelo\n","\n","O melhor regressor encontrado foi o **MLP**.\n","Os parâmetros utilizados no treinamento do modelo foram:\n","- activation: relu\n","- alpha: 0.0001\n","- batch_size: auto\n","- beta_1: 0.9\n","- beta_2: 0.999\n","- early_stopping: False\n","- epsilon: 1e-08\n","- hidden_layer_sizes: (100,)\n","- learning_rate: constant\n","- learning_rate_init: 0.001\n","- max_fun: 15000\n","- max_iter: 1000\n","- momentum: 0.9\n","- n_iter_no_change: 10\n","- nesterovs_momentum: True\n","- power_t: 0.5\n","- random_state: 42\n","- shuffle: True\n","- solver: adam\n","- tol: 0.0001\n","- validation_fraction: 0.1\n","- verbose: False\n","- warm_start: False\n"]}],"source":["\n","\n","# Importações de bibliotecas\n","from sklearn.datasets import load_diabetes # Para carregar o dataset de diabetes\n","from sklearn.model_selection import KFold # Para validação cruzada\n","from sklearn.preprocessing import StandardScaler # Para normalizar os dados\n","from sklearn.metrics import r2_score, mean_absolute_error # Métricas de regressão (R2 e MAE)\n","\n","# Regressores (modelos de regressão)\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.svm import SVR\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, GradientBoostingRegressor # XGBoosting como GradientBoostingRegressor do sklearn\n","import xgboost as xgb # Importação explícita para xgb.XGBRegressor, caso queira usar diretamente\n","import numpy as np # Para cálculos numéricos (média, etc.)\n","import pandas as pd # Para talvez exibir resultados em DataFrame (não usado diretamente na saída final, mas útil)\n","\n","# ---\n","# Carregar dados\n","X, y = load_diabetes(return_X_y=True)\n","\n","# Normalização dos dados\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# ---\n","# Modelos a serem avaliados (Regressores)\n","models = {\n","    \"Árvore de Regressão\": DecisionTreeRegressor(random_state=42),\n","    \"KNN\": KNeighborsRegressor(),\n","    \"SVR\": SVR(),\n","    \"MLP\": MLPRegressor(random_state=42, max_iter=1000), # Aumentado max_iter para auxiliar na convergência\n","    \"Random Forest\": RandomForestRegressor(random_state=42),\n","    \"Bagging\": BaggingRegressor(random_state=42),\n","    \"XGBoosting\": GradientBoostingRegressor(random_state=42) # Usando GradientBoostingRegressor do sklearn\n","    # Se quiser usar a biblioteca 'xgboost' diretamente, descomente a linha abaixo:\n","    # \"XGBoosting (lib)\": xgb.XGBRegressor(random_state=42, eval_metric='mae') # eval_metric para regressão\n","}\n","\n","# ---\n","# Função para treinar o modelo, calcular métricas e retornar resultados usando KFold\n","def evaluate_model_cv(model, X_data, y_data, n_splits=5, random_state=42):\n","    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n","    r2_scores = []\n","    mae_scores = []\n","\n","    # Importa clone aqui para garantir que está disponível na função\n","    from sklearn.base import clone\n","\n","    for train_index, test_index in kf.split(X_data):\n","        X_train, X_test = X_data[train_index], X_data[test_index]\n","        y_train, y_test = y_data[train_index], y_data[test_index]\n","\n","        # Clona o modelo para garantir que cada fold comece com um modelo 'novo'\n","        cloned_model = clone(model)\n","\n","        cloned_model.fit(X_train, y_train)\n","        y_pred = cloned_model.predict(X_test)\n","\n","        r2_scores.append(r2_score(y_test, y_pred))\n","        mae_scores.append(mean_absolute_error(y_test, y_pred))\n","\n","    # Retorna a média das métricas de todos os folds\n","    avg_r2 = np.mean(r2_scores)\n","    avg_mae = np.mean(mae_scores)\n","\n","    return avg_r2, avg_mae, model.get_params() # Retorna os parâmetros do modelo original\n","\n","# ---\n","# Avaliar todos os modelos\n","best_model_name = \"\"\n","best_r2 = -np.inf # Queremos maximizar R2, então começamos com um valor negativo infinito\n","best_mae = np.inf # Queremos minimizar MAE, então começamos com um valor positivo infinito\n","best_params = None\n","\n","all_results = [] # Para armazenar todos os resultados para a tabela final\n","\n","for model_name, model in models.items():\n","    avg_r2, avg_mae, params = evaluate_model_cv(model, X_scaled, y)\n","\n","    # Adiciona os resultados à lista de todos os resultados\n","    all_results.append({\n","        'Indutor': model_name,\n","        'Coeficiente de Determinação (R2)': avg_r2,\n","        'Erro Médio Absoluto (MAE)': avg_mae,\n","        'Parâmetros': params\n","    })\n","\n","    # Verifica se este é o melhor modelo até agora (baseado no R2)\n","    if avg_r2 > best_r2:\n","        best_r2 = avg_r2\n","        best_mae = avg_mae\n","        best_model_name = model_name\n","        best_params = params\n","\n","# ---\n","# Exibir a Tabela de Resultados completa\n","print(\"---\")\n","print(\"## Tabela de Resultados (Todos os Regressores)\")\n","print(\"\")\n","# Cabeçalho da tabela\n","print(\"| Indutor | Coeficiente de Determinação (R2) | Erro Médio Absoluto (MAE) |\")\n","print(\"|:--------------------|:------------------------------------|:--------------------------|\")\n","\n","# Linhas da tabela\n","for result in all_results:\n","    # Destaca o melhor modelo\n","    if result['Indutor'] == best_model_name:\n","        print(f\"| **{result['Indutor']}** | **{result['Coeficiente de Determinação (R2)']:.4f}** | **{result['Erro Médio Absoluto (MAE)']:.4f}** |\")\n","    else:\n","        print(f\"| {result['Indutor']} | {result['Coeficiente de Determinação (R2)']:.4f} | {result['Erro Médio Absoluto (MAE)']:.4f} |\")\n","\n","# ---\n","# Apresentar as informações do melhor modelo (B.1)\n","print(\"\\n---\")\n","print(\"## B.1) Parâmetros do Melhor Modelo\")\n","print(f\"\\nO melhor regressor encontrado foi o **{best_model_name}**.\")\n","print(\"Os parâmetros utilizados no treinamento do modelo foram:\")\n","for param, value in best_params.items():\n","    print(f\"- {param}: {value}\")"]}]}